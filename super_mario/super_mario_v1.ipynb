{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace532d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym-super-mario-bros==7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy\n",
    "import cv2\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from skimage import transform\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7258f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34848d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for t in range(2000):\n",
    "    action = env.action_space.sample()\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fc550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a758c5",
   "metadata": {},
   "source": [
    "# Preprocess Environment\n",
    "\n",
    "Environment data is returned to the agent in next_state. As you saw above, each state is represented by a [3, 240, 256] size array. Often that is more information than our agent needs; for instance, Mario’s actions do not depend on the color of the pipes or the sky!\n",
    "\n",
    "We use Wrappers to preprocess environment data before sending it to the agent.\n",
    "\n",
    "GrayScaleObservation is a common wrapper to transform an RGB image to grayscale; doing so reduces the size of the state representation without losing useful information. Now the size of each state: [1, 240, 256]\n",
    "\n",
    "ResizeObservation downsamples each observation into a square image. New size: [1, 84, 84]\n",
    "\n",
    "SkipFrame is a custom wrapper that inherits from gym.Wrapper and implements the step() function. Because consecutive frames don’t vary much, we can skip n-intermediate frames without losing much information. The n-th frame aggregates rewards accumulated over each skipped frame.\n",
    "\n",
    "FrameStack is a wrapper that allows us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can identify if Mario was landing or jumping based on the direction of his movement in the previous several frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        resize_obs = transform.resize(observation, self.shape)\n",
    "        # cast float back to uint8\n",
    "        resize_obs *= 255\n",
    "        resize_obs = resize_obs.astype(np.uint8)\n",
    "        return resize_obs\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3571ea",
   "metadata": {},
   "source": [
    "After applying the above wrappers to the environment, the final wrapped state consists of 4 gray-scaled consecutive frames stacked together. Each time Mario makes an action, the environment responds with a state of this structure. The structure is represented by a 3-D array of size [4, 84, 84].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f234639",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MarioNet, QPixelNetwork\n",
    "from agent import Agent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_dim=(4, 84, 84), action_dim=env.action_space.n, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaebef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# image_size = 84\n",
    "\n",
    "# def process_state(x):\n",
    "#     x = np.squeeze(x)\n",
    "#     x_resize = resize(x, (image_size,image_size,3), anti_aliasing=True)\n",
    "#     x_modified = Variable(torch.from_numpy(x_resize).float().to(device).view(3,image_size,image_size))\n",
    "#     return x_modified\n",
    "\n",
    "# def process_stack(frames):\n",
    "#     frames_list = list(frames)\n",
    "#     torch_stack = torch.stack(frames_list).unsqueeze(dim=0)\n",
    "#     torch_stack = torch_stack.view(torch_stack.size(0),torch_stack.size(2), torch_stack.size(1), torch_stack.size(3), torch_stack.size(4) )\n",
    "#     return torch_stack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state_copy= []\n",
    "\n",
    "def mario_dqn(episodes_num=40000, seed=0):\n",
    "    \n",
    "    scores = []\n",
    "    score_window = deque(maxlen=100)\n",
    "    #max_score = 7.0\n",
    "    for episode in tqdm(range(1, episodes_num+1)):\n",
    "        # reset the environment\n",
    "        state = env.reset()           # get the current state\n",
    "        score = 0\n",
    "        while True:\n",
    "            action = agent.act(state)        # select an action\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            score += reward                                # update the score\n",
    "            # add to replay\n",
    "            agent.cache(state, next_state, action, reward, done)\n",
    "            q, loss = agent.learn()\n",
    "            \n",
    "            state = next_state \n",
    "            \n",
    "            \n",
    "            # 10. Check if end of game\n",
    "            if done or info['flag_get']:\n",
    "                break\n",
    "        del q, loss\n",
    "            \n",
    "        score_window.append(score)\n",
    "        scores.append(score)\n",
    "\n",
    "        #print('\\rEpisode: {}\\tAverage Score: {:.2f}'.format(episode, np.mean(score_window)), end=\"\")\n",
    "        if episode % 100 == 0:\n",
    "            print('\\rEpisode: {}\\tAverage Score: {:.2f}'.format(episode, np.mean(score_window)))\n",
    "            torch.save(agent.net.state_dict(), 'checkpoints/checkpoint_mario_{}.pth'.format(episode))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d843789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = mario_dqn()\n",
    "#torch.save(agent.QN_local.state_dict(), 'checkpoint_pixels.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.net.load_state_dict(torch.load('checkpoints/checkpoint_mario_400.pth'))\n",
    "\n",
    "state = env.reset()\n",
    "score = 0\n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    env.render(mode=\"human\")\n",
    "    state, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "    if done or info['flag_get']:\n",
    "        break \n",
    "print(score)\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd636a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82b189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
